# ğŸš– Let's Take A Ride  
*Autonomous Taxi Navigation with Deep RL*  
*By Matthew Dall'Asen, Arpit Dang, Varun Satheesh*  

---

## ğŸ“– Overview  

The objective of this report is to develop an autonomous taxi agent capable of navigating between randomly selected start and end points within New York City. Traditional approaches have typically relied on rule-based algorithms for pathfinding, which often falter in dynamic or unpredictable urban environments. In contrast, this research aims to enhance the robustness and adaptability of autonomous navigation by leveraging Reinforcement Learning (RL).

By framing navigation as a sequential decision-making problem, we train an agent to optimize its route while balancing efficiency, safety, and adaptability. Using a simulated urban environment, the agent learns through interaction to make decisions that account for real-world complexity offering a significant advantage over static methods.

This report employs Deep Q-Networks (DQN), a model-free RL algorithm that enables scalable learning and effective approximation of optimal action-value functions. Moreover, the task is structured as goal-conditioned reinforcement learning, requiring the agent to condition its policy not only on its current state but also on a specified destination. This formulation allows for better generalization across a wide range of goals and environments, moving closer to the deployment of intelligent, self-navigating agents in real-world urban settings.


---

## ğŸ“Š Evaluation Summary  

| Metric                        | Value               |
|-------------------------------|---------------------|
| **Success Rate**              | 16/20 (80%)         |
| **Average Reward**            | 46.0                |
| **Avg. Distance Traveled**    | 0.0138              |
| **Avg. Steps per Episode**    | 16.3                |
| **Detour Ratio**              | 6.15                |
| **Step Efficiency**           | 0.0010 units/step   |
| **% Toward Goal**             | 67.8%               |

*Trained for 200 episodes, tested on 20 episodes*

---

## ğŸŒ† Environment Details  
**Core Specifications:**
- **Location:** 300m radius around NYC (40.7780, -73.9580)
- **Graph:** ~50 nodes (simplified from original 600)
- **Action Space:** Discrete (max 4 directions per node)
- **State Encoding:** Node indices + goal coordinates
- **Constraints:** One-way streets, episode limit (50 steps)

**Reward Structure:**
- `+10` for reaching goal
- `-1` per step
- `-0.5` for moving away from goal
- `-2` for repeated states

---

## ğŸ§  Learning Criteria & Framework

- Goal-conditioned, model-free RL agent
- Deep Q-Network (DQN) with linear and MLP layers
- State space: embedded vector of current node and goal
- Discrete action space: up to 4 valid neighboring nodes
- Epsilon-greedy exploration strategy
- Q-learning with target network for stability
- Experience replay buffer for batch updates
- Reward shaping based on:
  - Positive reward for goal completion
  - Negative penalties for revisits and suboptimal paths
  - Distance-based shaping rewards
- Episodes capped at 50 steps to prevent loops

## ğŸ“ Project Structure

```plaintext

lets-take-a-ride/
â”œâ”€â”€ data/                       # Cached OpenStreetMap data
â”‚   â””â”€â”€ *.graphml               # OSMnx preprocessed map files (shown as hashed filenames in screenshot)
â”œâ”€â”€ report/                     # Project report
â”‚   â”œâ”€â”€ report.pdf              # Final report
â”‚   â””â”€â”€ poster.jpg              # Project poster or visualization
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ envs/                   # Custom RL environment
â”‚   â”‚   â”œâ”€â”€ __pycache__/        # Python cache
â”‚   â”‚   â”œâ”€â”€ env.py              # Gymnasium environment logic
â”‚   â”‚   â””â”€â”€ env_data.py         # Preprocessing and environment utilities
â”‚   â”œâ”€â”€ execute/                # Main training and evaluation scripts
â”‚   â”‚   â”œâ”€â”€ __pycache__/        # Python cache
â”‚   â”‚   â”œâ”€â”€ train.py            # Training the agent
â”‚   â”‚   â”œâ”€â”€ test_model.py       # Evaluate a saved model
â”‚   â”‚   â””â”€â”€ inference.py        # Run inference / deployment
â”‚   â”œâ”€â”€ model/                  # RL algorithms
â”‚   â”‚   â”œâ”€â”€ __pycache__/        # Python cache
â”‚   â”‚   â”œâ”€â”€ DQN.py              # DQN implementation
â”‚   â”‚   â””â”€â”€ ActorCritic.py      # Actor-Critic model
â”‚   â””â”€â”€ utils/                  # Helper functions
â”‚       â”œâ”€â”€ __pycache__/        # Python cache
â”‚       â””â”€â”€ helper.py
â”œâ”€â”€ main.py                     # Entry point to train and evaluate the agent
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ trips/                      # GIFs of agent trajectories
â”œâ”€â”€ .gitignore                  # git ignore for data and other folders/files
â”œâ”€â”€ LICENSE                     # License file
â””â”€â”€ README.md                   # Project documentation

```

---

## â–¶ï¸ How to Use

Follow these steps to get started:

1. **Clone the repository**:
    ```bash
    git clone https://github.com/yourusername/lets-take-a-ride.git
    cd lets-take-a-ride
    ```

2. **Create a virtual environment (optional but recommended)**:
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3. **Install dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

4. **Run the main script**:
    ```bash
    python main.py
    ```

---

