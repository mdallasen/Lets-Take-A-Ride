# ğŸš– Let's Take A Ride  
*Autonomous Taxi Navigation with Deep RL*  
*By Matthew Dall'Asen, Arpit Dang, Varun Satheesh*  

---

## ğŸ“– Overview  
A reinforcement learning agent that learns to navigate a real-world inspired New York City street network using Goal-Conditioned Deep Q-Learning. The environment is built from OpenStreetMap data with realistic constraints like one-way streets and dynamic action spaces.

---

## ğŸ“Š Evaluation Summary  

| Metric                        | Value               |
|-------------------------------|---------------------|
| **Success Rate**              | 16/20 (80%)         |
| **Average Reward**            | 46.0                |
| **Avg. Distance Traveled**    | 0.0138              |
| **Avg. Steps per Episode**    | 16.3                |
| **Detour Ratio**              | 6.15                |
| **Step Efficiency**           | 0.0010 units/step   |
| **% Toward Goal**             | 67.8%               |

*Trained for 200 episodes, tested on 20 episodes*

---

## ğŸŒ† Environment Details  
**Core Specifications:**
- **Location:** 300m radius around NYC (40.7780, -73.9580)
- **Graph:** ~50 nodes (simplified from original 600)
- **Action Space:** Discrete (max 4 directions per node)
- **State Encoding:** Node indices + goal coordinates
- **Constraints:** One-way streets, episode limit (50 steps)

**Reward Structure:**
- `+10` for reaching goal
- `-1` per step
- `-0.5` for moving away from goal
- `-2` for repeated states

---

## ğŸ§  Learning Criteria & Framework

- Goal-conditioned, model-free RL agent
- Deep Q-Network (DQN) with linear and MLP layers
- State space: embedded vector of current node and goal
- Discrete action space: up to 4 valid neighboring nodes
- Epsilon-greedy exploration strategy
- Q-learning with target network for stability
- Experience replay buffer for batch updates
- Reward shaping based on:
  - Positive reward for goal completion
  - Negative penalties for revisits and suboptimal paths
  - Distance-based shaping rewards
- Episodes capped at 50 steps to prevent loops

## ğŸ“ Project Structure

```plaintext
lets-take-a-ride/
â”œâ”€â”€ data/                       # Cached OpenStreetMap data
â”‚   â”œâ”€â”€ *.graphml               # OSMnx preprocessed map files
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ envs/                   # Custom RL environment
â”‚   â”‚   â”œâ”€â”€ env.py              # Gymnasium environment logic
â”‚   â”‚   â””â”€â”€ env_data.py         # Preprocessing and environment utilities
â”‚   â”œâ”€â”€ execute/                # Main training and evaluation scripts
â”‚   â”‚   â”œâ”€â”€ train.py            # Training the agent
â”‚   â”‚   â”œâ”€â”€ test_model.py       # Evaluate a saved model
â”‚   â”‚   â””â”€â”€ inference.py        # Run inference / deployment
â”‚   â”œâ”€â”€ model/                  # DQN and other RL algorithms
â”‚   â”‚   â”œâ”€â”€ DQN.py              # Deep Q-Network implementation
â”‚   â”‚   â””â”€â”€ ActorCritic.py      # Optional: Actor-Critic model
â”‚   â””â”€â”€ utils/                  # Helper functions
â”‚       â””â”€â”€ helper.py
â”œâ”€â”€ main.py                     # Entry point to train and evaluate the agent
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ trips/                      # GIFs of agent trajectories
â”œâ”€â”€ .gitignore                  # git ignore for data and other folders / files
â”œâ”€â”€ poster.jpg                  # Project poster or visualization
â”œâ”€â”€ LICENSE                     # License file
â””â”€â”€ README.md                   # Project documentation
```

---

## â–¶ï¸ How to Use

Follow these steps to get started:

1. **Clone the repository**:
    ```bash
    git clone https://github.com/yourusername/lets-take-a-ride.git
    cd lets-take-a-ride
    ```

2. **Create a virtual environment (optional but recommended)**:
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3. **Install dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

4. **Run the main script**:
    ```bash
    python main.py
    ```

---

